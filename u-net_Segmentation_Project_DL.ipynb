{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"elapsed":1732478,"status":"error","timestamp":1714320119320,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"aPTdzjAG8wUU","outputId":"18100b1e-15bf-4b34-8ba9-9ceb6a3575f0"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"nbSGXFpd8tUq"},"source":["# U-Net standard model (with binary labels)"]},{"cell_type":"markdown","metadata":{"id":"ykhiYy2L8tUt"},"source":["Here you set all parameters thay you may need for training and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQh8eC4K8tUv"},"outputs":[],"source":["opts = {}\n","#opts['tf_version'] = 1.14                      # current version also works with tf 2.2\n","opts['imageType_train'] = '.tif'\n","opts['imageType_test'] = '.tif'\n","opts['number_of_channel'] = 3                   # Set if to '3' for RGB images and set it to '1' for grayscale images\n","opts['treshold'] = 0.5                          # treshold to convert the network output (stage 1) to binary masks\n","## input & output directories\n","opts['train_dir'] = '/content/drive/MyDrive/working/segmentation_images/tissue images/'\n","opts['train_label_dir'] = '/content/drive/MyDrive/working/segmentation_images/Annotator 1 (biologist)/mask binary/'\n","opts['train_label_masks'] = '/content/drive/MyDrive/working/segmentation_images/Annotator 1 (biologist)/label masks modify/'\n","opts['train_dis_dir'] = '/content/drive/MyDrive/working/segmentation_images/Annotator 1 (biologist)/distance maps/'\n","opts['results_save_path'] ='/content/drive/MyDrive/working/output'\n","opts['models_save_path'] ='/content/drive/MyDrive/working/output'\n","\n","opts['epoch_num_stage1'] = 7                   # number of epochs for stage 1\n","opts['quick_run'] = 0.01                         # step = (len(train)/batch_size) / quick_run (set it to large numbers just debugging the code)\n","opts['batch_size'] = 16                          # batch size\n","opts['random_seed_num'] = 19                    # keep it constant to be able to reproduce the results\n","opts['k_fold'] = 3                             # set to '1' to have no cross validation (much faster training but 2-3% degradation in performance)\n","opts['save_val_results'] = 1                    # set to '0' to skip saving the validation results in training\n","opts['init_LR'] = 0.001                         # initial learning rate for stage 1 and stage 2\n","opts['LR_decay_factor'] = 0.5                   # learning rate scheduler\n","opts['LR_drop_after_nth_epoch'] = 8            # learning rate scheduler\n","opts['crop_size'] = 512                         # crop size for training\n","opts['pretrained_model'] = 'efficientnetb0'     # future development\n","opts['use_pretrained_flag'] = 0                 # if you want to use a pretrained model in the encoder set it to one\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHJrZXr68tUy"},"outputs":[],"source":["## disabeling warning msg\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","# 0 = all messages are logged (default behavior)\n","# 1 = INFO messages are not printed\n","# 2 = INFO and WARNING messages are not printed\n","# 3 = INFO, WARNING, and ERROR messages are not printed\n","import warnings\n","warnings.simplefilter('ignore')\n","import sys\n","sys.stdout.flush() # resolving tqdm problem"]},{"cell_type":"markdown","metadata":{"id":"RFe9bm9G8tUz"},"source":["importing required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8N7NdWu8tU0"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import math\n","from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n","import random\n","\n","import keras\n","# from keras.utils import PyDataset\n","from keras.models import Model, load_model\n","from keras.layers import Input, BatchNormalization, Activation, add\n","from keras.layers import Dropout, Lambda\n","from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import concatenate\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras import backend as K\n","from keras.models import Model, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler,CSVLogger\n","\n","\n","###########################------------------------------------\n","\n","\n","# from tf.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from keras.optimizers import Adam\n","from keras.callbacks import LearningRateScheduler\n","from keras.losses import categorical_crossentropy\n","from keras.optimizers import Adam\n","#import segmentation_models as sm\n","from albumentations import*\n","import cv2\n","from random import shuffle                            #\n","import os\n","import matplotlib.pyplot as plt\n","from skimage.io import imsave\n","\n","\n","import time                                           # measuring training and test time\n","from glob import glob                                 # path control\n","import tqdm\n","from scipy.ndimage.morphology import binary_fill_holes\n","from skimage.morphology import remove_small_objects\n","from scipy.ndimage.filters import gaussian_filter\n","import skimage.morphology\n","from skimage import io, exposure, img_as_uint, img_as_float\n","from skimage.io import imsave, imread\n","from skimage.morphology import label\n","from skimage.segmentation import watershed\n","from skimage.feature import peak_local_max\n","#import segmentation_models as sm\n","from scipy import ndimage as ndi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714301952423,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"y2UHNYmh8tU3","outputId":"480768c1-a475-4746-cf7c-7d6798ce0cde"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.15.0'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"39Myk9BV8tU4"},"source":["defining functions that are used in training and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4A5nqs_8tU4"},"outputs":[],"source":["# Dice loss function\n","def dice_coef(y_true, y_pred):\n","    smooth = 1.\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1 - dice_coef(y_true, y_pred)\n","#####################################################################################\n","# Combination of Dice and binary cross entophy loss function\n","def bce_dice_loss(y_true, y_pred):\n","    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n","########################################################################################\n","# custom callsback (decaying learning rate)\n","def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n","    '''\n","    Wrapper function to create a LearningRateScheduler with step decay schedule.\n","    '''\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n","\n","    return LearningRateScheduler(schedule, verbose = 1)\n","#######################################################################################################\n","def binary_unet( IMG_CHANNELS, LearnRate):\n","    inputs = Input((None, None, IMG_CHANNELS))\n","    #s = Lambda(lambda x: x / 255) (inputs)\n","\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (inputs)\n","    c1 = Dropout(0.1) (c1)\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c1)\n","    p1 = MaxPooling2D((2, 2)) (c1)\n","\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p1)\n","    c2 = Dropout(0.1) (c2)\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c2)\n","    p2 = MaxPooling2D((2, 2)) (c2)\n","\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p2)\n","    c3 = Dropout(0.1) (c3)\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c3)\n","    p3 = MaxPooling2D((2, 2)) (c3)\n","\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p3)\n","    c4 = Dropout(0.1) (c4)\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c4)\n","    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p4)\n","    c5 = Dropout(0.1) (c5)\n","    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c5)\n","\n","    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n","    u6 = concatenate([u6, c4])\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u6)\n","    c6 = Dropout(0.1) (c6)\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c6)\n","\n","    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n","    u7 = concatenate([u7, c3])\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u7)\n","    c7 = Dropout(0.1) (c7)\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c7)\n","\n","    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n","    u8 = concatenate([u8, c2])\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u8)\n","    c8 = Dropout(0.1) (c8)\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c8)\n","\n","    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n","    u9 = concatenate([u9, c1], axis=3)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u9)\n","    c9 = Dropout(0.1) (c9)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c9)\n","\n","    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9) # for binary\n","\n","    model = Model(inputs=[inputs], outputs=[outputs])\n","    model.compile(optimizer = Adam(lr=LearnRate), loss= bce_dice_loss , metrics=[dice_coef]) #for binary\n","\n","    #model.summary()\n","    return model\n","#######################################################################################################\n","def deeper_binary_unet(IMG_CHANNELS, LearnRate):\n","    # Build U-Net model\n","    inputs = Input((None, None, IMG_CHANNELS))\n","    #s = Lambda(lambda x: x / 255) (inputs)\n","\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n","    c1 = Dropout(0.1) (c1)\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n","    p1 = MaxPooling2D((2, 2)) (c1)\n","\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n","    c2 = Dropout(0.1) (c2)\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n","    p2 = MaxPooling2D((2, 2)) (c2)\n","\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n","    c3 = Dropout(0.1) (c3)\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n","    p3 = MaxPooling2D((2, 2)) (c3)\n","\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n","    c4 = Dropout(0.1) (c4)\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n","    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","\n","    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n","    c4_new = Dropout(0.1) (c4_new)\n","    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4_new)\n","    p4_new = MaxPooling2D(pool_size=(2, 2)) (c4_new)\n","\n","    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4_new)\n","    c5 = Dropout(0.1) (c5)\n","    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n","\n","\n","    u6_new = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c5)\n","    u6_new = concatenate([u6_new, c4_new])\n","    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6_new)\n","    c6_new = Dropout(0.1) (c6_new)\n","    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6_new)\n","\n","    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6_new)\n","    u6 = concatenate([u6, c4])\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n","    c6 = Dropout(0.1) (c6)\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n","\n","    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n","    u7 = concatenate([u7, c3])\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n","    c7 = Dropout(0.1) (c7)\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n","\n","    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n","    u8 = concatenate([u8, c2])\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n","    c8 = Dropout(0.1) (c8)\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n","\n","    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n","    u9 = concatenate([u9, c1], axis=3)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n","    c9 = Dropout(0.1) (c9)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n","\n","    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n","\n","    model_deeper = Model(inputs=[inputs], outputs=[outputs])\n","    model_deeper.compile(optimizer = Adam(lr=LearnRate), loss= bce_dice_loss , metrics=[ dice_coef])\n","    #model_deeper.summary()\n","    return model_deeper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTC6HvtO8tU8"},"outputs":[],"source":["# augmentation function\n","def albumentation_aug(p=1.0, crop_size_row = 448, crop_size_col = 448 ):\n","    return Compose([\n","        RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1),\n","        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n","        RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, brightness_by_max=True, p=0.4),\n","        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.1),\n","        HorizontalFlip(always_apply=False, p=0.5),\n","        VerticalFlip(always_apply=False, p=0.5),\n","        RandomRotate90(always_apply=False, p=0.5),\n","        #ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.3),\n","    ], p=p) # --> this p has the second proiroty comapred to the p inside each argument (e.g. HorizontalFlip(always_apply=False, p=0.5) )\n","###########################################################\n","def albumentation_aug_light(p=1.0, crop_size_row = 448, crop_size_col = 448):\n","    return Compose([\n","        RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1.0),\n","        HorizontalFlip(always_apply=False, p=0.5),\n","        VerticalFlip(always_apply=False, p=0.5),\n","        RandomRotate90(always_apply=False, p=0.5),\n","        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.1),\n","    ], p=p, additional_targets={'mask1': 'mask','mask2': 'mask'}) # --> this p has the second proiroty comapred to the p inside each argument (e.g. HorizontalFlip(always_apply=False, p=0.5) )\n"]},{"cell_type":"markdown","metadata":{"id":"vRuSLTH_8tU-"},"source":["* evaluation indexes (from the hovernet paper: https://github.com/vqdang/hover_net/blob/master/src/metrics/stats_utils.py)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nwfoWTA8tU_"},"outputs":[],"source":["def get_dice_1(true, pred):\n","    \"\"\"\n","        Traditional dice\n","    \"\"\"\n","    # cast to binary 1st\n","    true = np.copy(true)\n","    pred = np.copy(pred)\n","    true[true > 0] = 1\n","    pred[pred > 0] = 1\n","    inter = true * pred\n","    denom = true + pred\n","    return 2.0 * np.sum(inter) / np.sum(denom)\n","##############################################################################################\n","def get_fast_aji(true, pred):\n","    \"\"\"\n","    AJI version distributed by MoNuSeg, has no permutation problem but suffered from\n","    over-penalisation similar to DICE2\n","    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4]\n","    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no\n","    effect on the result.\n","    \"\"\"\n","    true = np.copy(true) # ? do we need this\n","    pred = np.copy(pred)\n","    true_id_list = list(np.unique(true))\n","    pred_id_list = list(np.unique(pred))\n","\n","    true_masks = [None,]\n","    for t in true_id_list[1:]:\n","        t_mask = np.array(true == t, np.uint8)\n","        true_masks.append(t_mask)\n","\n","    pred_masks = [None,]\n","    for p in pred_id_list[1:]:\n","        p_mask = np.array(pred == p, np.uint8)\n","        pred_masks.append(p_mask)\n","\n","    # prefill with value\n","    pairwise_inter = np.zeros([len(true_id_list) -1,\n","                               len(pred_id_list) -1], dtype=np.float64)\n","    pairwise_union = np.zeros([len(true_id_list) -1,\n","                               len(pred_id_list) -1], dtype=np.float64)\n","\n","    # caching pairwise\n","    for true_id in true_id_list[1:]: # 0-th is background\n","        t_mask = true_masks[true_id]\n","        pred_true_overlap = pred[t_mask > 0]\n","        pred_true_overlap_id = np.unique(pred_true_overlap)\n","        pred_true_overlap_id = list(pred_true_overlap_id)\n","        for pred_id in pred_true_overlap_id:\n","            if pred_id == 0: # ignore\n","                continue # overlaping background\n","            p_mask = pred_masks[pred_id]\n","            total = (t_mask + p_mask).sum()\n","            inter = (t_mask * p_mask).sum()\n","            pairwise_inter[true_id-1, pred_id-1] = inter\n","            pairwise_union[true_id-1, pred_id-1] = total - inter\n","    #\n","    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n","    # pair of pred that give highest iou for each true, dont care\n","    # about reusing pred instance multiple times\n","    paired_pred = np.argmax(pairwise_iou, axis=1)\n","    pairwise_iou = np.max(pairwise_iou, axis=1)\n","    # exlude those dont have intersection\n","    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n","    paired_pred = paired_pred[paired_true]\n","    # print(paired_true.shape, paired_pred.shape)\n","    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n","    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n","    #\n","    paired_true = (list(paired_true + 1)) # index to instance ID\n","    paired_pred = (list(paired_pred + 1))\n","    # add all unpaired GT and Prediction into the union\n","    unpaired_true = np.array([idx for idx in true_id_list[1:] if idx not in paired_true])\n","    unpaired_pred = np.array([idx for idx in pred_id_list[1:] if idx not in paired_pred])\n","    for true_id in unpaired_true:\n","        overall_union += true_masks[true_id].sum()\n","    for pred_id in unpaired_pred:\n","        overall_union += pred_masks[pred_id].sum()\n","    #\n","    aji_score = overall_inter / overall_union\n","    return aji_score\n","##############################################################################################\n","def remap_label(pred, by_size=False):\n","    \"\"\"\n","    Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3]\n","    not [0, 2, 4, 6]. The ordering of instances (which one comes first)\n","    is preserved unless by_size=True, then the instances will be reordered\n","    so that bigger nucler has smaller ID\n","    Args:\n","        pred    : the 2d array contain instances where each instances is marked\n","                  by non-zero integer\n","        by_size : renaming with larger nuclei has smaller id (on-top)\n","    \"\"\"\n","    pred_id = list(np.unique(pred))\n","    pred_id.remove(0)\n","    if len(pred_id) == 0:\n","        return pred # no label\n","    if by_size:\n","        pred_size = []\n","        for inst_id in pred_id:\n","            size = (pred == inst_id).sum()\n","            pred_size.append(size)\n","        # sort the id by size in descending order\n","        pair_list = zip(pred_id, pred_size)\n","        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n","        pred_id, pred_size = zip(*pair_list)\n","\n","    new_pred = np.zeros(pred.shape, np.int32)\n","    for idx, inst_id in enumerate(pred_id):\n","        new_pred[pred == inst_id] = idx + 1\n","    return new_pred\n","\n","##############################################################################################\n","def get_fast_pq(true, pred, match_iou=0.5):\n","    \"\"\"\n","    `match_iou` is the IoU threshold level to determine the pairing between\n","    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n","    if IoU > `match_iou`. However, pair of `p` and `g` must be unique\n","    (1 prediction instance to 1 GT instance mapping).\n","    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n","    in bipartite graphs) is caculated to find the maximal amount of unique pairing.\n","    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n","    the number of pairs is also maximal.\n","    Fast computation requires instance IDs are in contiguous orderding\n","    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand\n","    and `by_size` flag has no effect on the result.\n","    Returns:\n","        [dq, sq, pq]: measurement statistic\n","        [paired_true, paired_pred, unpaired_true, unpaired_pred]:\n","                      pairing information to perform measurement\n","    \"\"\"\n","    assert match_iou >= 0.0, \"Cant' be negative\"\n","\n","    true = np.copy(true)\n","    pred = np.copy(pred)\n","    true_id_list = list(np.unique(true))\n","    pred_id_list = list(np.unique(pred))\n","\n","    true_masks = [None, ]\n","    for t in true_id_list[1:]:\n","        t_mask = np.array(true == t, np.uint8)\n","        true_masks.append(t_mask)\n","\n","    pred_masks = [None, ]\n","    for p in pred_id_list[1:]:\n","        p_mask = np.array(pred == p, np.uint8)\n","        pred_masks.append(p_mask)\n","\n","    # prefill with value\n","    pairwise_iou = np.zeros([len(true_id_list) - 1,\n","                             len(pred_id_list) - 1], dtype=np.float64)\n","\n","    # caching pairwise iou\n","    for true_id in true_id_list[1:]:  # 0-th is background\n","        t_mask = true_masks[true_id]\n","        pred_true_overlap = pred[t_mask > 0]\n","        pred_true_overlap_id = np.unique(pred_true_overlap)\n","        pred_true_overlap_id = list(pred_true_overlap_id)\n","        for pred_id in pred_true_overlap_id:\n","            if pred_id == 0:  # ignore\n","                continue  # overlaping background\n","            p_mask = pred_masks[pred_id]\n","            total = (t_mask + p_mask).sum()\n","            inter = (t_mask * p_mask).sum()\n","            iou = inter / (total - inter)\n","            pairwise_iou[true_id - 1, pred_id - 1] = iou\n","    #\n","    if match_iou >= 0.5:\n","        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n","        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n","        paired_true, paired_pred = np.nonzero(pairwise_iou)\n","        paired_iou = pairwise_iou[paired_true, paired_pred]\n","        paired_true += 1  # index is instance id - 1\n","        paired_pred += 1  # hence return back to original\n","    else:  # * Exhaustive maximal unique pairing\n","        #### Munkres pairing with scipy library\n","        # the algorithm return (row indices, matched column indices)\n","        # if there is multiple same cost in a row, index of first occurence\n","        # is return, thus the unique pairing is ensure\n","        # inverse pair to get high IoU as minimum\n","        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n","        ### extract the paired cost and remove invalid pair\n","        paired_iou = pairwise_iou[paired_true, paired_pred]\n","\n","        # now select those above threshold level\n","        # paired with iou = 0.0 i.e no intersection => FP or FN\n","        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n","        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n","        paired_iou = paired_iou[paired_iou > match_iou]\n","\n","    # get the actual FP and FN\n","    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n","    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n","    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n","\n","    #\n","    tp = len(paired_true)\n","    fp = len(unpaired_pred)\n","    fn = len(unpaired_true)\n","    # get the F1-score i.e DQ\n","    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n","    # get the SQ, no paired has 0 iou so not impact\n","    sq = paired_iou.sum() / (tp + 1.0e-6)\n","\n","    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51RGu26T8tVC"},"outputs":[],"source":["# other useful finction for training\n","\n","def get_id_from_file_path(file_path, indicator):\n","    return file_path.split(os.path.sep)[-1].replace(indicator, '')\n","############################################################\n","def chunker(seq, seq2, size):\n","    return ([seq[pos:pos + size], seq2[pos:pos + size]] for pos in range(0, len(seq), size))\n","############################################################\n","def data_gen_heavy(list_files, list_files2, batch_size, p , size_row, size_col, distance_unet_flag = 0, augment=False, BACKBONE_model = 'efficientnetb0', use_pretrain_flag =1):\n","    #preprocess_input = sm.get_preprocessing(BACKBONE_model)\n","    crop_size_row = size_row\n","    crop_size_col = size_col\n","    aug = albumentation_aug(p, crop_size_row, crop_size_col)\n","\n","    while True:\n","        #shuffle(list_files)\n","        for batch in chunker(list_files,list_files2, batch_size):\n","            #X = [cv2.resize(cv2.imread(x), (size, size)) for x in batch]\n","            X = []\n","            Y = []\n","\n","            for count in range(len(batch[0])):\n","                # x = cv2.resize(cv2.imread(batch[0][count]), (size_col, size_row))\n","                # x_mask = cv2.resize(cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE), (size_col, size_row))\n","                x = cv2.imread(batch[0][count])\n","                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n","                x_mask = cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE)\n","\n","                x_mask_temp = np.zeros((x_mask.shape[0], x_mask.shape[1]))\n","                x_mask_temp[x_mask == 255] = 1\n","\n","\n","                if distance_unet_flag == False:\n","                    if augment:\n","                        augmented = aug(image= x, mask= x_mask_temp)\n","                        x = augmented['image']\n","                        if use_pretrain_flag == 1:\n","                            x = preprocess_input(x)\n","                        x_mask_temp = augmented['mask']\n","                        x = x/255\n","                    X.append(x)\n","                    Y.append(x_mask_temp)\n","                    #imsave('/media/masih/wd/projects/MoNuSAC_binary/results/images/an/{}_binary.png'.format(get_id_from_file_path(batch[0][count], '.png')), x_mask_epithelial)\n","                    #imsave('/media/masih/wd/projects/MoNuSAC_binary/results/images/an/{}.png'.format(get_id_from_file_path(batch[0][count], '.tif')), x)\n","                else:\n","                    if augment:\n","                        augmented = aug(image=x, mask=x_mask)\n","                        x = augmented['image']\n","                        if use_pretrain_flag == 1:\n","                            x = preprocess_input(x)\n","                        x_mask = augmented['mask']\n","\n","                    X.append(x)\n","                    x_mask = (x_mask - np.min(x_mask))/ (np.max(x_mask) - np.min(x_mask) + 0.0000001)\n","                    Y.append(x_mask)\n","\n","                del x_mask\n","                del x_mask_temp\n","                del x\n","            Y = np.expand_dims(np.array(Y), axis=3)\n","            Y = np.array(Y)\n","            yield np.array(X), np.array(Y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kC97_TeJ8tVE"},"outputs":[],"source":["# create folders to save the best models and images (if needed) for each fold\n","if not os.path.exists('/content/drive/MyDrive/working/output/images/'):\n","    os.makedirs('/content/drive/MyDrive/working/output/images/')\n","if not os.path.exists('/content/drive/MyDrive/working/output/models/'):\n","    os.makedirs('/content/drive/MyDrive/working/output/models/')\n","if not os.path.exists(opts['results_save_path']+ 'stage1/validation/pure_unet'):\n","    os.makedirs(opts['results_save_path'] + 'stage1/validation/pure_unet')\n","if not os.path.exists(opts['results_save_path']+ 'stage1/validation/watershed_unet'):\n","    os.makedirs(opts['results_save_path'] + 'stage1/validation/watershed_unet')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714301954176,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"hFRN0YUv8tVE","outputId":"6fb98a28-5b07-43d2-cea4-250c9d50c75e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of training images: 30\n"]}],"source":["train_files = glob('{}*{}'.format(opts['train_dir'], opts['imageType_train']))\n","train_files_mask = glob('{}*.png'.format(opts['train_label_dir']))\n","train_files_dis = glob('{}*.png'.format(opts['train_dis_dir']))\n","train_files_labels = glob('{}*.tif'.format(opts['train_label_masks']))\n","\n","\n","train_files.sort()\n","train_files_mask.sort()\n","train_files_dis.sort()\n","train_files_labels.sort()\n","print(\"Total number of training images:\", len(train_files))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714301954176,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"_86WuGSB8tVF","outputId":"90f1a29d-e656-489c-b78f-761d3fd69efb"},"outputs":[{"data":{"text/plain":["['/content/drive/MyDrive/working/segmentation_images/tissue images/Human_AdrenalGland_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_AdrenalGland_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_AdrenalGland_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Larynx_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Larynx_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Larynx_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_LymphNodes_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_LymphNodes_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_LymphNodes_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Mediastinum_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Mediastinum_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Mediastinum_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Pancreas_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Pancreas_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Pancreas_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Pleura_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Pleura_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Pleura_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Skin_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Skin_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Skin_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Testes_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Testes_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Testes_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Thymus_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Thymus_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_Thymus_03.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_ThyroidGland_01.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_ThyroidGland_02.tif',\n"," '/content/drive/MyDrive/working/segmentation_images/tissue images/Human_ThyroidGland_03.tif']"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# we have 10 organ in this dataset\n","train_files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714301955647,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"aR-eSHTl8tVG","outputId":"40cd266c-d325-43e0-a22d-520d8493c036"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of each fold: 6\n"]}],"source":["# creating 10 folds to perfrom 10 fold cross-validation (for each fold images from the 9 organs are used for training and the images from one organ are used as validation)\n","\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold1 = train_files[0: int(np.round(len(train_files) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold\" + str(k + 1)] = train_files[int(np.round(len (train_files) / opts['k_fold']) * k): int(np.round(len(train_files) / opts['k_fold']) * (k+1))]\n","print(\"length of each fold:\", len(fold1))\n","\n","# for binary mask\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold_mask1 = train_files_mask[0: int(np.round(len(train_files_mask) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold_mask\" + str(k + 1)] = train_files_mask[int(np.round(len (train_files_mask) / opts['k_fold']) * k): int(np.round(len(train_files_mask) / opts['k_fold']) * (k+1))]\n","\n","# for distance mask\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold_dis1 = train_files_dis[0: int(np.round(len(train_files_dis) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold_dis\" + str(k + 1)] = train_files_dis[int(np.round(len (train_files_dis) / opts['k_fold']) * k): int(np.round(len(train_files_dis) / opts['k_fold']) * (k+1))]\n","\n","# for label masks (just for evaluation)\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold_label1 = train_files_labels[0: int(np.round(len(train_files_labels) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold_label\" + str(k + 1)] = train_files_labels[int(np.round(len (train_files_labels) / opts['k_fold']) * k): int(np.round(len(train_files_labels) / opts['k_fold']) * (k+1))]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"F3VjikjQ8tVH","outputId":"862967a3-212c-4373-f884-0c4c75e3e8db"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 1/10\n","99/99 [==============================] - ETA: 0s - loss: -0.4135 - dice_coef: 0.6305\n","Epoch 1: val_dice_coef improved from -inf to 0.63738, saving model to /content/drive/MyDrive/working/outputraw_unet_1.weights.h5\n","99/99 [==============================] - 89s 807ms/step - loss: -0.4135 - dice_coef: 0.6305 - val_loss: -0.4591 - val_dice_coef: 0.6374 - lr: 0.0010\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 2/10\n","99/99 [==============================] - ETA: 0s - loss: -0.5461 - dice_coef: 0.7290\n","Epoch 2: val_dice_coef improved from 0.63738 to 0.70329, saving model to /content/drive/MyDrive/working/outputraw_unet_1.weights.h5\n","99/99 [==============================] - 61s 616ms/step - loss: -0.5461 - dice_coef: 0.7290 - val_loss: -0.5511 - val_dice_coef: 0.7033 - lr: 0.0010\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 3/10\n","99/99 [==============================] - ETA: 0s - loss: -0.5725 - dice_coef: 0.7454\n","Epoch 3: val_dice_coef improved from 0.70329 to 0.70356, saving model to /content/drive/MyDrive/working/outputraw_unet_1.weights.h5\n","99/99 [==============================] - 61s 616ms/step - loss: -0.5725 - dice_coef: 0.7454 - val_loss: -0.5580 - val_dice_coef: 0.7036 - lr: 0.0010\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 4/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6118 - dice_coef: 0.7684\n","Epoch 4: val_dice_coef improved from 0.70356 to 0.73924, saving model to /content/drive/MyDrive/working/outputraw_unet_1.weights.h5\n","99/99 [==============================] - 61s 616ms/step - loss: -0.6118 - dice_coef: 0.7684 - val_loss: -0.6043 - val_dice_coef: 0.7392 - lr: 0.0010\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 5/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6403 - dice_coef: 0.7857\n","Epoch 5: val_dice_coef improved from 0.73924 to 0.76131, saving model to /content/drive/MyDrive/working/outputraw_unet_1.weights.h5\n","99/99 [==============================] - 61s 612ms/step - loss: -0.6403 - dice_coef: 0.7857 - val_loss: -0.6472 - val_dice_coef: 0.7613 - lr: 0.0010\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 6/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6608 - dice_coef: 0.7981\n","Epoch 6: val_dice_coef improved from 0.76131 to 0.77760, saving model to /content/drive/MyDrive/working/outputraw_unet_1.weights.h5\n","99/99 [==============================] - 61s 613ms/step - loss: -0.6608 - dice_coef: 0.7981 - val_loss: -0.6588 - val_dice_coef: 0.7776 - lr: 0.0010\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 7/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6804 - dice_coef: 0.8103\n","Epoch 7: val_dice_coef did not improve from 0.77760\n","99/99 [==============================] - 59s 593ms/step - loss: -0.6804 - dice_coef: 0.8103 - val_loss: -0.6622 - val_dice_coef: 0.7765 - lr: 0.0010\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 8/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6936 - dice_coef: 0.8183\n","Epoch 8: val_dice_coef did not improve from 0.77760\n","99/99 [==============================] - 59s 596ms/step - loss: -0.6936 - dice_coef: 0.8183 - val_loss: -0.5995 - val_dice_coef: 0.7488 - lr: 0.0010\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 0.0005.\n","Epoch 9/10\n","99/99 [==============================] - ETA: 0s - loss: -0.7183 - dice_coef: 0.8335\n","Epoch 9: val_dice_coef did not improve from 0.77760\n","99/99 [==============================] - 59s 593ms/step - loss: -0.7183 - dice_coef: 0.8335 - val_loss: -0.6454 - val_dice_coef: 0.7766 - lr: 5.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 0.0005.\n","Epoch 10/10\n","99/99 [==============================] - ETA: 0s - loss: -0.7326 - dice_coef: 0.8422\n","Epoch 10: val_dice_coef did not improve from 0.77760\n","99/99 [==============================] - 59s 593ms/step - loss: -0.7326 - dice_coef: 0.8422 - val_loss: -0.6053 - val_dice_coef: 0.7596 - lr: 5.0000e-04\n","6/6 [==============================] - 0s 17ms/step\n","==========\n","average dice pure Unet for fold0: 0.7809819695803237\n","average AJI pure Unet for fold0: 0.47968535106595045\n","average PQ pure Unet for fold0: 0.4189533333828664\n","==========\n","==========\n","average Dice Unet watershed for fold0: 0.7793234770136476\n","average AJI Unet watershed for fold0: 0.4748001022851778\n","average PQ Unet watershed for fold0: 0.38170693240443304\n","==========\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 1/10\n","99/99 [==============================] - ETA: 0s - loss: -0.4385 - dice_coef: 0.6310\n","Epoch 1: val_dice_coef improved from -inf to 0.72129, saving model to /content/drive/MyDrive/working/outputraw_unet_2.weights.h5\n","99/99 [==============================] - 69s 610ms/step - loss: -0.4385 - dice_coef: 0.6310 - val_loss: -0.4304 - val_dice_coef: 0.7213 - lr: 0.0010\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 2/10\n","99/99 [==============================] - ETA: 0s - loss: -0.5669 - dice_coef: 0.7278\n","Epoch 2: val_dice_coef did not improve from 0.72129\n","99/99 [==============================] - 59s 596ms/step - loss: -0.5669 - dice_coef: 0.7278 - val_loss: -0.4321 - val_dice_coef: 0.7139 - lr: 0.0010\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 3/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6131 - dice_coef: 0.7582\n","Epoch 3: val_dice_coef improved from 0.72129 to 0.73946, saving model to /content/drive/MyDrive/working/outputraw_unet_2.weights.h5\n","99/99 [==============================] - 65s 654ms/step - loss: -0.6131 - dice_coef: 0.7582 - val_loss: -0.5105 - val_dice_coef: 0.7395 - lr: 0.0010\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 4/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6337 - dice_coef: 0.7716\n","Epoch 4: val_dice_coef improved from 0.73946 to 0.77008, saving model to /content/drive/MyDrive/working/outputraw_unet_2.weights.h5\n","99/99 [==============================] - 66s 666ms/step - loss: -0.6337 - dice_coef: 0.7716 - val_loss: -0.5197 - val_dice_coef: 0.7701 - lr: 0.0010\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 5/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6621 - dice_coef: 0.7895\n","Epoch 5: val_dice_coef improved from 0.77008 to 0.77977, saving model to /content/drive/MyDrive/working/outputraw_unet_2.weights.h5\n","99/99 [==============================] - 65s 656ms/step - loss: -0.6621 - dice_coef: 0.7895 - val_loss: -0.5561 - val_dice_coef: 0.7798 - lr: 0.0010\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 6/10\n","99/99 [==============================] - ETA: 0s - loss: -0.6866 - dice_coef: 0.8060\n","Epoch 6: val_dice_coef did not improve from 0.77977\n","99/99 [==============================] - 59s 595ms/step - loss: -0.6866 - dice_coef: 0.8060 - val_loss: -0.5187 - val_dice_coef: 0.7685 - lr: 0.0010\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n","Epoch 7/10\n","86/99 [=========================>....] - ETA: 7s - loss: -0.7132 - dice_coef: 0.8233"]}],"source":["# main training loop (for all 10 fold cross-validation)\n","start_time = time.time()\n","dice_pure_unet = np.zeros([opts['k_fold'],len(fold1)])\n","AJI_pure_unet = np.zeros([opts['k_fold'],len(fold1)])\n","PQ_pure_unet = np.zeros([opts['k_fold'],len(fold1)])\n","\n","dice_unet_watershed = np.zeros([opts['k_fold'],len(fold1)])\n","AJI_unet_watershed = np.zeros([opts['k_fold'],len(fold1)])\n","PQ_unet_watershed = np.zeros([opts['k_fold'],len(fold1)])\n","\n","\n","\n","for K_fold in range(opts['k_fold']):\n","    train = []\n","    train_mask = []\n","    train_dis = []\n","\n","    val = eval('fold' + str(K_fold + 1))\n","    val_mask = eval('fold_mask' + str(K_fold + 1))\n","    val_dis = eval('fold_dis' + str(K_fold + 1))\n","    val_label = eval('fold_label' + str(K_fold + 1))\n","\n","    for ii in range(opts['k_fold']):\n","        if ii != K_fold:\n","            train = eval('fold' + str(ii + 1)) + train\n","\n","    for ii in range(opts['k_fold']):\n","        if ii != K_fold:\n","            train_mask = eval('fold_mask' + str(ii + 1)) + train_mask\n","\n","    for ii in range(opts['k_fold']):\n","        if ii != K_fold:\n","            train_dis = eval('fold_dis' + str(ii + 1)) + train_dis\n","\n","    if opts['k_fold'] == 1: # for no cross validation the training will be with all training images\n","        train = train_files\n","        train_mask = train_files_mask\n","        train_dis = train_files_dis\n","\n","    random.Random(opts['random_seed_num']).shuffle(train)\n","    random.Random(opts['random_seed_num']).shuffle(train_mask)\n","    random.Random(opts['random_seed_num']).shuffle(train_dis)\n","\n","\n","    ## creating validation data for each fold (just for evaluation)\n","    # it is not included in the main training loop for a faster training\n","    validation_X = []\n","    validation_Y = []\n","    validation_DIS = []\n","    if len(val)<200: # memory consideration\n","        for an in range(len(val)):\n","            x = cv2.imread(val[an])\n","            x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n","\n","            aug = albumentation_aug_light(1, opts['crop_size'], opts['crop_size'])\n","            #augmented = aug(image=x)\n","            #x = augmented['image']\n","            if opts['use_pretrained_flag'] == 1:\n","                x = preprocess_input(x)\n","            img_mask = imread(val_label[an])\n","            x = x/255\n","            validation_X.append(x)\n","            validation_Y.append(img_mask)\n","\n","    else:\n","        for an in range(200):\n","            x = cv2.imread(val[an])\n","            x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n","\n","            aug = albumentation_aug_light(1, opts['crop_size'], opts['crop_size'])\n","            #augmented = aug(image=x)\n","            #x = augmented['image']\n","            if opts['use_pretrained_flag'] ==1:\n","                x = preprocess_input(x)\n","            img_mask = imread(val_label[an])\n","            x = x/255\n","            validation_X.append(x)\n","            validation_Y.append(img_mask)\n","\n","    validation_X = np.array(validation_X)\n","    validation_Y = np.array(validation_Y)\n","\n","\n","    model_path = opts['models_save_path'] + 'raw_unet_{}.weights.h5'.format(K_fold+1)\n","    logger = CSVLogger(opts['models_save_path']+ 'raw_unet_{}.log'.format(K_fold + 1))\n","    LR_drop = step_decay_schedule(initial_lr= opts['init_LR'], decay_factor = opts['LR_decay_factor'], epochs_drop = opts['LR_drop_after_nth_epoch'])\n","    model_raw = deeper_binary_unet(opts['number_of_channel'], opts['init_LR'])\n","    checkpoint = ModelCheckpoint(model_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max', save_weights_only = True)\n","\n","    # training\n","    history = model_raw.fit_generator(data_gen_heavy(train,\n","                                                     train_mask,\n","                                                     opts['batch_size'],\n","                                                     1,\n","                                                     opts['crop_size'], opts['crop_size'],\n","                                                     distance_unet_flag=0,\n","                                                     augment=True,\n","                                                     BACKBONE_model=opts['pretrained_model'],\n","                                                     use_pretrain_flag=opts['use_pretrained_flag']),\n","                                      validation_data=data_gen_heavy(val,\n","                                                                     val_mask,\n","                                                                     opts['batch_size'],\n","                                                                     1,\n","                                                                     opts['crop_size'], opts['crop_size'],\n","                                                                     distance_unet_flag=0,\n","                                                                     augment=True,\n","                                                                     BACKBONE_model=opts['pretrained_model'],\n","                                                                     use_pretrain_flag=opts['use_pretrained_flag']),\n","                                      validation_steps=1,\n","                                      epochs=opts['epoch_num_stage1'], verbose=1,\n","                                      callbacks=[checkpoint, logger, LR_drop],\n","                                      steps_per_epoch=(len(train) // opts['batch_size']) // opts['quick_run'])\n","\n","    model_raw.load_weights(opts['models_save_path'] + 'raw_unet_{}.weights.h5'.format(K_fold + 1))\n","\n","    ## predication on validation set\n","    preds_val = model_raw.predict(validation_X, verbose=1, batch_size=1)\n","    preds_val_t = (preds_val > opts['treshold']).astype(np.uint8)\n","\n","\n","    for val_len in range(len(preds_val)):\n","        # with watershed post processing\n","        local_maxi = peak_local_max(np.squeeze(preds_val[val_len]), indices=False,exclude_border=False, footprint=np.ones((15, 15)))\n","        markers = ndi.label(local_maxi)[0]\n","        labels = watershed(-np.squeeze(preds_val[val_len]), markers,mask = np.squeeze(preds_val_t[[val_len]]))\n","        labels[np.squeeze(preds_val_t[[val_len]])==0] = 0\n","\n","        # without post processing\n","        pred = np.squeeze(preds_val_t[val_len])\n","        label_pred = skimage.morphology.label(pred)\n","\n","        label_pred = remap_label(label_pred)\n","        validation_Y[val_len] = remap_label(validation_Y[val_len])\n","        labels = remap_label(labels)\n","\n","        imsave(opts['results_save_path'] + 'stage1/validation/watershed_unet/{}.png'.format(get_id_from_file_path(val[val_len], opts['imageType_train'])),labels.astype(np.uint16))\n","        imsave(opts['results_save_path'] + 'stage1/validation/pure_unet/{}.png'.format(get_id_from_file_path(val[val_len], opts['imageType_train'])),label_pred.astype(np.uint16))\n","\n","\n","\n","        dice_pure_unet[K_fold, val_len]= get_dice_1(validation_Y[val_len], label_pred)\n","        AJI_pure_unet[K_fold, val_len] = get_fast_aji(validation_Y[val_len], label_pred,)\n","        PQ_pure_unet[K_fold, val_len] = get_fast_pq(validation_Y[val_len], label_pred,)[0][2]\n","\n","        dice_unet_watershed[K_fold, val_len]= get_dice_1(validation_Y[val_len],labels, )\n","        AJI_unet_watershed[K_fold, val_len] = get_fast_aji(validation_Y[val_len], labels)\n","        PQ_unet_watershed[K_fold, val_len]  = get_fast_pq(validation_Y[val_len], labels)[0][2]\n","\n","\n","    print('==========')\n","    print('average dice pure Unet for fold{}:'.format(K_fold), np.mean(dice_pure_unet[K_fold, :]))\n","    print('average AJI pure Unet for fold{}:'.format(K_fold), np.mean(AJI_pure_unet[K_fold, :]))\n","    print('average PQ pure Unet for fold{}:'.format(K_fold), np.mean(PQ_pure_unet[K_fold, :]))\n","\n","    print('==========')\n","\n","    print('==========')\n","    print('average Dice Unet watershed for fold{}:'.format(K_fold), np.mean(dice_unet_watershed[K_fold, :]))\n","    print('average AJI Unet watershed for fold{}:'.format(K_fold), np.mean(AJI_unet_watershed[K_fold, :]))\n","    print('average PQ Unet watershed for fold{}:'.format(K_fold), np.mean(PQ_unet_watershed[K_fold, :]))\n","    print('==========')\n","finish_time = time.time()\n","print('==========')\n","print('total training time (all 10 folds):',  (finish_time- start_time)/60, 'minutes')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGIAZwAUV46K"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":700,"status":"error","timestamp":1714301072943,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"KjemDZF48tVI","outputId":"590d7752-0e31-46cc-cc4b-dccd54ef9d8e"},"outputs":[{"ename":"ValueError","evalue":"All arrays must be of the same length","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-430d4ba02661>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m organ_name = ['Human_AdrenalGland', 'Human_Larynx', 'Human_LymphNodes', 'Human_Mediastinum',\n\u001b[1;32m      3\u001b[0m               'Human_Pancreas','Human_Pleura', 'Human_Skin', 'Human_Testes' , 'Human_Thymus', 'Human_ThyroidGland']\n\u001b[0;32m----> 4\u001b[0;31m df = pd.DataFrame({'Oragn': organ_name, 'DICE mean': np.mean(dice_pure_unet, axis = 1),\n\u001b[0m\u001b[1;32m      5\u001b[0m                    \u001b[0;34m'AJI mean'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAJI_pure_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    \u001b[0;34m'PQ mean'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPQ_pure_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"]}],"source":["import pandas as pd\n","organ_name = ['Human_AdrenalGland', 'Human_Larynx', 'Human_LymphNodes', 'Human_Mediastinum',\n","              'Human_Pancreas','Human_Pleura', 'Human_Skin', 'Human_Testes' , 'Human_Thymus', 'Human_ThyroidGland']\n","df = pd.DataFrame({'Oragn': organ_name, 'DICE mean': np.mean(dice_pure_unet, axis = 1),\n","                   'AJI mean': np.mean(AJI_pure_unet, axis = 1),\n","                   'PQ mean': np.mean(PQ_pure_unet, axis = 1)\n","                  })\n","df.to_csv('final_scores_pure_unet.csv', index=False)\n","print('averge overall dice score (pure Unet):',\"{:.2f}\".format(np.mean(dice_pure_unet)*100), '%')\n","print('averge overall AJI score (pure Unet):', \"{:.2f}\".format(np.mean(AJI_pure_unet)*100), '%')\n","print('averge overall PQ score (pure Unet):', \"{:.2f}\".format(np.mean(PQ_pure_unet)*100), '%')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":417,"status":"error","timestamp":1714301079376,"user":{"displayName":"Friday V","userId":"05543200725873389140"},"user_tz":-330},"id":"FenAaUVT8tVJ","outputId":"42bc4399-fe18-4c92-e160-4ff70e90e422"},"outputs":[{"ename":"ValueError","evalue":"All arrays must be of the same length","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-69de23a2f865>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m organ_name = ['Human_AdrenalGland', 'Human_Larynx', 'Human_LymphNodes', 'Human_Mediastinum',\n\u001b[1;32m      3\u001b[0m               'Human_Pancreas','Human_Pleura', 'Human_Skin', 'Human_Testes' , 'Human_Thymus', 'Human_ThyroidGland']\n\u001b[0;32m----> 4\u001b[0;31m df = pd.DataFrame({'Oragn': organ_name, 'DICE mean': np.mean(dice_unet_watershed, axis = 1),\n\u001b[0m\u001b[1;32m      5\u001b[0m                    \u001b[0;34m'AJI mean'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAJI_unet_watershed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    \u001b[0;34m'PQ mean'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPQ_unet_watershed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"]}],"source":["import pandas as pd\n","organ_name = ['Human_AdrenalGland', 'Human_Larynx', 'Human_LymphNodes', 'Human_Mediastinum',\n","              'Human_Pancreas','Human_Pleura', 'Human_Skin', 'Human_Testes' , 'Human_Thymus', 'Human_ThyroidGland']\n","df = pd.DataFrame({'Oragn': organ_name, 'DICE mean': np.mean(dice_unet_watershed, axis = 1),\n","                   'AJI mean': np.mean(AJI_unet_watershed, axis = 1),\n","                   'PQ mean': np.mean(PQ_unet_watershed, axis = 1),\n","                  })\n","df.to_csv('final_scores_unet_watershed.csv', index=False)\n","print('averge overall dice score (Unet + watershed):', \"{:.2f}\".format(np.mean(dice_unet_watershed)*100),'%')\n","print('averge overall AJI score (Unet + watershed):', \"{:.2f}\".format(np.mean(AJI_unet_watershed)*100),'%')\n","print('averge overall PQ score (Unet + watershed):', \"{:.2f}\".format(np.mean(PQ_unet_watershed)*100),'%')\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enXgrNai8tVJ"},"outputs":[],"source":["np.save('/content/drive/MyDrive/working/output/dice_pure_unet.npy', dice_pure_unet)\n","np.save('/content/drive/MyDrive/working/output/AJI_pure_unet.npy', AJI_pure_unet)\n","np.save('/content/drive/MyDrive/working/output/PQ_pure_unet.npy', PQ_pure_unet)\n","\n","np.save('/content/drive/MyDrive/working/output/dice_unet_watershed.npy', dice_unet_watershed)\n","np.save('/content/drive/MyDrive/working/output/AJI_unet_watershed.npy', AJI_unet_watershed)\n","np.save('/content/drive/MyDrive/working/output/PQ_unet_watershed.npy', PQ_unet_watershed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1lteAPe8tVK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":693124,"sourceId":1900145,"sourceType":"datasetVersion"}],"dockerImageVersionId":29955,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}